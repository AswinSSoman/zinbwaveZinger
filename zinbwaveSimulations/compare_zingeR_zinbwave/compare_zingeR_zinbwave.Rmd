---
title: "zingeR versus zinbwave weights"
author: "Fanny Perraudeau"
date: "`r Sys.Date()`"
output: 
  html_document: 
    fig_height: 7
    fig_width: 7
    toc: yes
    code_folding: hide
    toc_float: yes
---

```{r options, echo=FALSE, results="hide",mesasge=FALSE, error=FALSE, include=FALSE, autodep=TRUE}
knitr::opts_chunk$set(fig.align="center", cache=TRUE, error=FALSE, message=FALSE, warning=TRUE)
library(zinbwave)
library(BiocParallel)
library(doParallel)
library(zingeR)
library(Biobase)
library(gamlss)
library(gamlss.tr)
library(edgeR)
library(scales)
library(DESeq2)
library(iCOBRA) # roc
library(limma)
library(genefilter) #filtered pvalues
library(MAST)
library(RColorBrewer)
library(ggplot2)
library(wesanderson)
```

```{r}
NCORES <- 2
registerDoParallel(NCORES)
register(DoparParam())
```

```{r help}
pvalueAdjustment_kvdb <- function(baseMean, filter, pValue,
                             theta, alpha=0.05, pAdjustMethod="BH") {
  # perform independent filtering
    if (missing(filter)) {
      filter <- baseMean
    }
    if (missing(theta)) {
      lowerQuantile <- mean(filter == 0)
      if (lowerQuantile < .95) upperQuantile <- .95 else upperQuantile <- 1
      theta <- seq(lowerQuantile, upperQuantile, length=50)
    }

    # do filtering using genefilter
    stopifnot(length(theta) > 1)
    filtPadj <- filtered_p(filter=filter, test=pValue,
                           theta=theta, method=pAdjustMethod)
    numRej  <- colSums(filtPadj < alpha, na.rm = TRUE)
    # prevent over-aggressive filtering when all genes are null,
    # by requiring the max number of rejections is above a fitted curve.
    # If the max number of rejection is not greater than 10, then don't
    # perform independent filtering at all.
    lo.fit <- lowess(numRej ~ theta, f=1/5)
    if (max(numRej) <= 10) {
      j <- 1
    } else {
      residual <- if (all(numRej==0)) {
        0
      } else {
        numRej[numRej > 0] - lo.fit$y[numRej > 0]
      }
      thresh <- max(lo.fit$y) - sqrt(mean(residual^2))
      j <- if (any(numRej > thresh)) {
        which(numRej > thresh)[1]
      } else {
        1
      }
    }
    padj <- filtPadj[, j, drop=TRUE]
    cutoffs <- quantile(filter, theta)
    filterThreshold <- cutoffs[j]
    filterNumRej <- data.frame(theta=theta, numRej=numRej)
    filterTheta <- theta[j]

    return(list(padj=padj, filterThreshold=filterThreshold, filterTheta=filterTheta, filterNumRej = filterNumRej, lo.fit=lo.fit, alpha=alpha))

}
```

# Simulating scRNA-seq data
## Real dataset used

The scRNA-seq simulation is based on the Islam mouse dataset, which compares 48 embryonic stem cells to 44 embryonic fibroblasts in mouse. Reference is Saiful Islam, Una Kjallquist, Annalena Moliner, Pawel Zajac, Jian-Bing Fan, Peter Lonnerberg, and Sten Linnarsson. Characterization of the single-cell transcriptional landscape by highly multiplex RNA-seq. Genome research.

## Simulation procedure

Counts are simulated using a ZINB distribution. The parameters of the ZINB are estimated genewise but accounting for cell level log CPM. Extracted from zingeR vignette:

"The zingeR package provides a framework to simulate scRNA-seq data. The user can input a real scRNA-seq dataset to extract feature-level parameters for generating scRNA-seq expression counts. The `getDatasetZTNB` function will estimate empirical mean expression proportions for the positive counts $\hat \lambda_g = \frac{1}{|\{ y_{gi} > 0\}|} \sum_{i \in \{ y_{gi} > 0\}} y_{gi}/N_i$, with $|x|$ denoting the cardinality of the set $x$, $y_{gi}$ the  expression count for gene $g$ in sample $i$ and $N_i$ the library size (sequencing depth) of sample $i$. The function also estimates dispersions $\phi_g$ based on the zero-truncated negative binomial (ZTNB) distribution.  Additionally, the function derives empirical probabilities on zero counts $p_{gi}$. Aside from these feature-specific parameters, `getDatasetZTNB` also estimates the global association of $p_{gi}$ with (a smooth function of) the average log CPM $A_g$ of a gene and the library size $N_i$."

```{r data}
data(islamEset, package = "zingeR")
islamHlp=exprs(islamEset)[9:2008,] #first 8 are spike-ins.
cellType=pData(islamEset)[,"cellType"]
paramsIslam = getDatasetZTNB(counts = islamHlp, 
                             design = model.matrix(~ cellType))
```

"Next we use the parameters to simulate the expression counts. Library sizes for the simulated samples are by default resampled from the input dataset but they can also be specified by the user.
The simulation framework models positive and zero counts separately similar to a hurdle model.
The zero probability $p_{gi}$ of a gene $g$ is modelled as a function of its expression intensity (in terms of average log counts per million $A_g$) and the sequencing depth $N_i$ of the sample $i$ using a semiparametric additive logistic regression model.
The simulation paradigm jointly samples the gene-wise estimated parameters $\{\hat \lambda_{gi}, \hat \phi_g, A_g, p_{gi}\}$ to retain gene specific characteristics present in the original dataset.
Positive counts are then simulated according to a ZTNB distribution with mean $\hat \mu_{gi} = \hat \lambda_{gi} N_i$ and dispersion $\hat \phi_g$.
We use the expected probability on zero counts $p_g = \frac{\sum_{i=1}^n p_{gi}}{n}$ to introduce zero counts by simulating from a binomial process. 
The simulation thus acknowledges both gene-specific characteristics as well as broad associations across all genes and provides realistic scRNA-seq data."

```{r sims}
nSamples=80
grp=as.factor(rep(0:1, each = nSamples/2)) #two-group comparison
nTags=2000 #nr of features
set.seed(436)
DEind = sample(1:nTags,floor(nTags*.10),replace=FALSE) #10% DE
fcSim=(2 + rexp(length(DEind), rate = 1/2)) #fold changes
libSizes=sample(colSums(islamHlp),nSamples,replace=TRUE) #library sizes
simDataIslam <- NBsimSingleCell(foldDiff = fcSim, ind = DEind,
                                dataset = islamHlp, nTags = nTags,
                                group = grp,
                                verbose = TRUE, params = paramsIslam,
                                lib.size = libSizes)
simDataIslam$counts[1:5,1:5]

# BCV plots
dOrig=suppressWarnings(edgeR::calcNormFactors(DGEList(islamHlp)))
dOrig=estimateGLMTagwiseDisp(estimateGLMCommonDisp(dOrig, design=model.matrix(~cellType), interval=c(0,10)),prior.df=0)

d=suppressWarnings(edgeR::calcNormFactors(DGEList(simDataIslam$counts)))
d=estimateGLMTagwiseDisp(estimateGLMCommonDisp(d, design=model.matrix(~grp), interval=c(0,10)),prior.df=0)

par(mfrow=c(1,2))
plotBCV(dOrig,ylim=c(0,13), xlim=c(4,16))
plotBCV(d,ylim=c(0,13), xlim=c(4,16))
par(mfrow=c(1,1))

# association of library size with zeros
plot(x=log(colSums(islamHlp)), y=colMeans(islamHlp==0), xlab="Log library size", ylab="Fraction of zeros", xlim=c(5.5,13))
points(x=log(colSums(simDataIslam$counts)), y=colMeans(simDataIslam$counts==0), col=2)

# association of aveLogCPM with zeros
plot(x=edgeR::aveLogCPM(islamHlp), y=rowMeans(islamHlp==0), xlab="Average log CPM", ylab="Fraction of zeros", ylim=c(0,1), col=alpha(1,1/2), pch=19, cex=.3)
points(x=edgeR::aveLogCPM(simDataIslam$counts), y=rowMeans(simDataIslam$counts==0),col=alpha(2,1/2),pch=19,cex=.3)
```

# Methods
## RNA-seq methods
### edgeR
```{r}
edgeR <- function(counts, group){
  d <- DGEList(counts)
  d <- suppressWarnings(edgeR::calcNormFactors(d))
  design <- model.matrix(~group)
  d <- estimateDisp(d, design)
  plotBCV(d)
  fit <- glmFit(d,design)
  lrt <- glmLRT(fit, coef = 2)
  pval <- lrt$table$PValue
  logFC <- lrt$table$logFC
  padj <- p.adjust(pval, "BH")
  cbind(pval = pval, padj = padj, logFC = logFC)
}
```

### DESeq2
```{r}
DESeq2 <- function(counts, group){
  colData <- data.frame(group = group)
  dse <- DESeqDataSetFromMatrix(countData=counts, colData=colData, design=~group)
  colData(dse)$group <- as.factor(colData(dse)$group)
  dse <- DESeq2::estimateSizeFactors(dse, type="poscounts")
  dse <- estimateDispersions(dse)
  dse <- nbinomWaldTest(dse, betaPrior=TRUE)
  rr <- results(dse)
  logFC <- rr$log2FoldChange
  cbind(pval = rr$pvalue, padj = rr$padj, logFC = logFC)
}
```

### limma-voom
```{r}
limma <- function(counts, group){
	design <- model.matrix(~ group)
	nf <- suppressWarnings(edgeR::calcNormFactors(counts))
	y <- voom(counts, design, plot = FALSE, lib.size = colSums(counts) * nf)
	fit <- lmFit(y, design)
	fit <- eBayes(fit)
	tt <- topTable(fit, coef = 2, n = nrow(counts), sort.by = "none")
	pval <- tt$P.Value
	padj <- tt$adj.P.Val
	logFC <- tt$logFC
	cbind(pval = pval, padj = padj, logFC = logFC)
}
```

## zingeR

Counts are modelled as ZINB. Weights are posterior probabilities that a count belongs to the count component given that the count and library size is observed. Parameters are estimated using EM algorithm. The main difference with zinbwave is that zingeR pi depends only on the sample (pi_i) and not on the sample and gene (pi_{ij}).

### zingeR-edgeR
```{r}
zingeR_edgeR <- function(counts, group){
  d <- DGEList(counts)
  d <- suppressWarnings(edgeR::calcNormFactors(d))
  design <- model.matrix(~ group)
  weights <- zeroWeightsLS(counts = d$counts, design = design, maxit = 200,
                           normalization = "TMM", verbose = FALSE)
  d$weights <- weights
  d <- estimateDisp(d, design)
  plotBCV(d)
  fit <- glmFit(d,design)
  lrt <- glmWeightedF(fit, coef = 2, independentFiltering = TRUE)
  cbind(pval = lrt$table$PValue, padj =lrt$table$padjFilter, 
        logFC = lrt$table$logFC)
}
```

### zingeR-DESeq2
```{r}
zingeR_DESeq2 <- function(counts, group){
  colData <- data.frame(group = group)
  design <- model.matrix(~ group)
  dse <- DESeqDataSetFromMatrix(countData = counts, colData = colData,
                                design = ~group)
  weights <- zeroWeightsLS(counts = counts, design = design, maxit = 200,
                           normalization = "DESeq2_poscounts", colData = colData,
                           designFormula = ~group, verbose = F)
  assays(dse)[["weights"]] <- weights
  dse <- DESeq2::estimateSizeFactors(dse, type="poscounts")
  dse <- estimateDispersions(dse)
  dse <- nbinomWaldTest(dse, betaPrior = TRUE, useT = TRUE,
                        df = rowSums(weights) - 2)
  rr <- results(dse)
  cbind(pval = rr$pvalue, padj = rr$padj, logFC = rr$log2FoldChange)
}
```

### zingeR-limma-voom-filtered
```{r}
zingeR_limma <- function(counts, group){
  design <- model.matrix(~group)
  nf <- edgeR::calcNormFactors(counts)
  zeroWeights <- zeroWeightsLS(counts=counts, design=design, maxit = 200,
                               verbose = FALSE)
  y <- voom(counts, design, plot=FALSE, lib.size = colSums(counts)*nf,
            weights = zeroWeights)
  y$weights <- y$weights*zeroWeights
  fit <- lmFit(y, design, weights=y$weights)
  fit$df.residual <- rowSums(zeroWeights) - ncol(design)
  fit <- eBayes(fit)
  tt <- topTable(fit,coef=2,n=nrow(counts), sort.by = "none")
  pval <- tt$P.Value
  baseMean = unname(rowMeans(sweep(counts,2,nf,FUN="*")))
  hlp <- pvalueAdjustment_kvdb(baseMean=baseMean, pValue=pval)
  padj <- hlp$padj
  logFC <- tt$logFC
  cbind(pval = pval, padj = padj, logFC = logFC)
}
```

## zinbwave

We compute the same weights as zingeR (i.e. posterior probabilities that a count belongs to the count component given that the count and library size is observed), but using our estimation procedure.

```{r zinbwaveWeights}
computeZinbwaveWeights <- function(zinb, counts){
  mu <- getMu(zinb)
  pi <- getPi(zinb)
  theta <- getTheta(zinb)
  theta_mat <- matrix(rep(theta, each = ncol(counts)), ncol = nrow(counts))
  nb_part <- dnbinom(t(counts), size = theta_mat, mu = mu)
  zinb_part <- pi * ( t(counts) == 0 ) + (1 - pi) *  nb_part
  zinbwg <- ( (1 - pi) * nb_part ) / zinb_part 
  t(zinbwg)
}
```

### zinbwave-edgeR
```{r}
zinbwave_edgeR <- function(counts, group, zinb){
  d=DGEList(counts)
  d=suppressWarnings(calcNormFactors(d))
  design=model.matrix(~group)
  weights <- computeZinbwaveWeights(zinb, d$counts)
  d$weights <- weights
  d=estimateDisp(d, design)
  plotBCV(d)
  fit=glmFit(d,design)
  lrt=glmWeightedF(fit, coef=2, independentFiltering = TRUE,
                   filter = rowMedians(fit$fitted.values))
  cbind(pval = lrt$table$PValue, padj = lrt$table$padjFilter,
        logFC = lrt$table$logFC)
}
```

### zinbwave-DESeq2
```{r}
zinbwave_DESeq2 <- function(counts, group, zinb){
  colData=data.frame(group=group)
  design=model.matrix(~group)
  dse=DESeqDataSetFromMatrix(countData=counts, colData=colData, design=~group)
  weights <- computeZinbwaveWeights(zinb, d$counts)
  assays(dse)[["weights"]]=weights
  dse = DESeq2::estimateSizeFactors(dse, type="poscounts")
  dse = estimateDispersions(dse)
  dse = nbinomWaldTest(dse, betaPrior=TRUE, useT=TRUE, df=rowSums(weights)-2)
  res = results(dse)
  cbind(pval = res$pvalue, padj = res$padj, logFC = res$log2FoldChange)
}
```

### zinbwave-limma-voom
```{r}
zinbwave_limma <- function(counts, group, zinb){
  design <- model.matrix(~group)
  nf <- edgeR::calcNormFactors(counts)
  zeroWeights <- computeZinbwaveWeights(zinb, d$counts)
  y <- voom(counts, design, plot=FALSE, lib.size = colSums(counts)*nf,
            weights = zeroWeights)
  y$weights <- zeroWeights
  fit <- lmFit(y, design, weights=y$weights)
  fit$df.residual <- rowSums(zeroWeights) - ncol(design)
  fit <- eBayes(fit)
  tt <- topTable(fit,coef=2,n=nrow(counts), sort.by = "none")
  pval <- tt$P.Value
  baseMean = unname(rowMeans(sweep(counts,2,nf,FUN="*")))
  hlp <- pvalueAdjustment_kvdb(baseMean=baseMean, pValue=pval)
  padj <- hlp$padj
  logFC <- tt$logFC
  cbind(pval = pval, padj = padj, logFC = logFC)
}
```

```{r core}
core <- SummarizedExperiment(simDataIslam$counts,
                             colData = data.frame(grp = grp))
```

```{r zinbcommondisp}
#zinb_c <- zinbFit(core, X = '~ grp', commondispersion = TRUE)
#save(zinb_c, file = 'zinb-common-disp.rda')
load('zinb-common-disp.rda')
```

```{r zinbgenewisedisp}
#zinb_g <- zinbFit(core, X = '~ grp', commondispersion = FALSE)
#save(zinb_g, file = 'zinb-genewise-disp.rda')
load('zinb-genewise-disp.rda')
```


## scRNA-seq methods
### MAST

Hurdle model. Groups are in the discrete and continuous parts. Percentage of zeros is used as cell level covariate.

```{r}
MAST <- function(counts, group){
  tpm <- counts * 1e6 / colSums(counts) 
  sca <- FromMatrix(tpm, cData = data.frame(group = group))
  ngeneson <- apply(exprs(sca), 1, function(x) mean(x > 0))
  CD <- cData(sca)
  CD$ngeneson <- ngeneson
  CD$cngeneson <- CD$ngeneson - mean(ngeneson)
  cData(sca) <- CD
  fit <- zlm(~ group + ngeneson, sca = sca)
  lrFit <- lrTest(fit, 'group')
  pval <- lrFit[, 'hurdle', 'Pr(>Chisq)']
  padj <- p.adjust(pval, method = "BH")
  cbind(pval = pval, padj = padj, logFC = NA)
}
```


# Results

## Biological coefficient of variation (BCV)

I don't understand well the relationship between BCV and mean. BCV is more uniform compared to mean than zingeR. It also seems results are better when BCV is bigger when average is smaller (see FPR_mocks.Rmd). But is it really better?
```{r, warning=FALSE}
counts = simDataIslam$counts
group = grp
myfct = list(DESeq2 = DESeq2, 
             edgeR = edgeR, 
             limma = limma,
             MAST = MAST,
             zingeR_DESeq2 = zingeR_DESeq2,
             zingeR_edgeR = zingeR_edgeR,
             zingeR_limma = zingeR_limma)
             
resList = lapply(myfct, function(fct){
  fct(counts = counts, group = grp)
})
resList[['zinBwave_c_DESeq2']] = zinbwave_DESeq2(counts, grp, zinb_c)
resList[['zinBwave_c_edgeR']]  = zinbwave_edgeR(counts, grp, zinb_c)
resList[['zinBwave_c_limma']]  = zinbwave_limma(counts, grp, zinb_c)
resList[['zinBwave_g_DESeq2']] = zinbwave_DESeq2(counts, grp, zinb_g)
resList[['zinBwave_g_edgeR']]  = zinbwave_edgeR(counts, grp, zinb_g)
resList[['zinBwave_g_limma']]  = zinbwave_limma(counts, grp, zinb_g)
```

## pval histograms
```{r}
res = lapply(resList, as.data.frame)
```

```{r histogramPVAL}
hist = lapply(1:length(res), function(i){
  hist(res[[i]][,'pval'], main = names(res)[i])
})
```

## Volcano plots
```{r volcanos}
trueDE = rep(0, nTags)
trueDE[simDataIslam$indDE] = 1

volcanos <- lapply(1:length(res), function(i){
  plot(res[[i]][, 'logFC'], -log10(res[[i]][, 'padj']),
       pch = 20, col = 'gray', cex = .5, xlim = c(-10, 10), ylim = c(0, 15),
       ylab = '-log10(pvalue)', xlab = 'logFC', main = names(res)[i])
  points(res[[i]][simDataIslam$indDE, 'logFC'], pch=1, col=3, cex=.5, lwd=1,
          -log10(res[[i]][simDataIslam$indDE, 'padj']))
})
```

```{r}
lapply(resList, function(x){
  sum(is.na(x[, 'padj']))
})
```

## nDE, TPR, FDR at pval=0.05
```{r nb}
lapply(res, function(y){
  nDE = sum(y$padj <= 0.05, na.rm = TRUE)
  TPR = mean(simDataIslam$indDE %in% which( y$padj <= 0.05))
  FPR = mean(which(y$padj <= 0.05) %in% simDataIslam$indNonDE)
  c(nDE = nDE, TPR = TPR, FPR = FPR)
})
```

## TPR vs FDR
```{r plot}
mycol = c(brewer.pal(11, "RdYlGn")[1:3], rev(brewer.pal(11, "RdYlBu"))[1:3], 
          brewer.pal(8, "Dark2")[4], rev(brewer.pal(11, "PiYG"))[1:3],
          brewer.pal(11, "BrBG")[1:3])
mycol = c(mycol[1:length(res)], 'black')
names(mycol) = c(names(res), 'truth')
  
pp = COBRAData(pval = as.data.frame(do.call(cbind, lapply(res, '[[', 1))),
               padj = as.data.frame(do.call(cbind, lapply(res, '[[', 2))),
               truth = data.frame(status = trueDE))
cobraperf <- calculate_performance(pp, binary_truth = "status", thrs = 0.05)
colsCobra <- mycol[match(sort(names(cobraperf@overlap)[1:(ncol(cobraperf@overlap)-1)]), names(mycol))]
cobraplot <- prepare_data_for_plot(cobraperf, colorscheme = colsCobra)
plot_fdrtprcurve(cobraplot, plottype = c("curve", "points"), pointsize = .1,
                 linewidth = .5)
```

```{r}
kk = names(cobraperf@overlap)[1:(ncol(cobraperf@overlap)-1)]
colsCobra <- mycol[match(sort(kk), names(mycol))]
cobraplot <- prepare_data_for_plot(cobraperf, colorscheme = colsCobra,
                                   keepmethods = names(res)[c(4:6, 8:9, 11:12)])
plot_fdrtprcurve(cobraplot, plottype = c("curve", "points"), pointsize = .1,
                 xaxisrange = c(0, 0.2),linewidth = .5)
```

# Discussion
## Additional notes on zinbwave

Let's check that zinbwave works as expected
```{r zinbunsup,eval=FALSE}
print(system.time(zinb <- zinbFit(core, K = 2, X = '~ grp')))
plot(getW(zinb), col = grp, main = 'supervised W')
```

```{r zinbsup,eval=FALSE}
print(system.time(zinb <- zinbFit(core, K = 2)))
```

It works great. Groups are exactly recovered.
```{r plotWunsup,eval=FALSE}
W = getW(zinb)
km = kmeans(W, centers = 2, iter.max = 1000, nstart = 1000)
clusters = km$cluster
par(mfrow=c(1,2))
plot(W, col = grp, main = 'unsupervised W, col = truth')
plot(W, col = clusters, main = 'unsupervised W, col = kmeans')
table(clusters, grp)
```

## Compare zinbwave and zingeR weights

```{r}
d=DGEList(simDataIslam$counts)
d=suppressWarnings(calcNormFactors(d))
design=model.matrix(~grp)
zingeR_weights <- zeroWeightsLS(counts=d$counts, design=design, maxit=200,
                         normalization="TMM", verbose = F)
```

```{r}
zinbwave_c_weights <- computeZinbwaveWeights(zinb_c, counts)
zinbwave_g_weights <- computeZinbwaveWeights(zinb_g, counts)
```

```{r}
ylim=c(0,100000)
par(mfrow=c(1,2))
hist(zingeR_weights, main='zingeR',ylim=ylim,xlab='Weights')
hist(zinbwave_c_weights, main ='zinbwave (common disp.)',ylim=ylim,
     xlab='Weights')
par(mfrow=c(1,1))

qqplot(zinbwave_c_weights, zinbwave_g_weights,
       main ='QQplot zinbwave weights - genewise versus common disp.')
abline(a=0,b=1)
```

```{r}
mm = (zinbwave_c_weights + zingeR_weights)/2
dd = zinbwave_c_weights - zingeR_weights
smoothScatter(mm, dd, xlab = 'Mean', ylab = 'Difference', 
              main='Mean-Difference Plot\nzinbwave versus zingeR weights')
abline(b = 0, a = 0)
```


```{r}
par(mfrow=c(1,2))
smoothScatter(rowMeans(counts==0), rowMeans(zinbwave_c_weights),
              xlab = 'Percentage of zeros (per gene)',
              ylab = 'Mean of weights (per gene)',
              main = 'zinbwave')
smoothScatter(rowMeans(counts==0), rowMeans(zingeR_weights),
              xlab = 'Percentage of zeros (per gene)', 
              ylab = 'Mean of weights (per gene)',
              main = 'zingeR')
par(mfrow=c(1,1))
```


```{r}
par(mfrow=c(1,2))
smoothScatter(rowMeans(counts==0), rowMedians(zinbwave_c_weights),
              xlab = 'Percentage of zeros (per gene)',
              ylab = 'Median of weights (per gene)',
              main = 'zinbwave')
smoothScatter(rowMeans(counts==0), rowMedians(zingeR_weights),
              xlab = 'Percentage of zeros (per gene)', 
              ylab = 'Median of weights (per gene)',
              main = 'zingeR')
par(mfrow=c(1,1))
```

