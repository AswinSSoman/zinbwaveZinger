---
title: "Simulations from Islam dataset, fold change 2"
author: "Fanny Perraudeau & Koen Van den Berge"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_height: 7
    fig_width: 7
    toc: yes
    code_folding: hide
    toc_float: yes
---

This document uses Fanny's original simulation code in `islam_sims_fc2.Rmd' and has been adapted by Koen to adopt the new simulation framework.

```{r options, echo=FALSE, results="hide",message=FALSE, error=FALSE, include=FALSE, autodep=TRUE, warning=FALSE}
setwd("/Users/koenvandenberge/Dropbox/phdKoen/singleCell/zinbwaveZingerGithub/zinbwaveZinger/zinbwaveSimulations/islam_sims_fc2/")
knitr::opts_chunk$set(fig.align="center", cache=TRUE, error=FALSE, message=FALSE, warning=TRUE)
library(zinbwave)
library(BiocParallel)
library(doParallel)
library(Biobase)
library(edgeR)
library(scales)
library(DESeq2)
library(iCOBRA) # roc
library(limma)
library(genefilter) #filtered pvalues
library(MAST)
library(RColorBrewer)
library(knitr)
library(ggplot2)
library(cowplot)
# use new simulation.
#library(zingeR)
pathToParentFolder="~/Dropbox/phdKoen/singleCell/zinbwavezingerGitHub/zinbwaveZinger/"
source(paste0(pathToParentFolder,"zingeRsimulationFunctions/simulationHelpFunctions_v7_diffInZero.R"))
```


```{r cores}
NCORES <- 2
registerDoParallel(NCORES)
register(DoparParam())
```

The goal of this document is to reproduce Figure 1 from our paper. A scRNA-seq dataset from Islam dataset is simulated using zingeR simulation framework. We evaluate the performance of different DE methods.

# Simulate scRNA-seq data

## Real dataset

The scRNA-seq simulation is based on the Islam mouse dataset, which compares 48 embryonic stem cells to 44 embryonic fibroblasts in mouse. Reference is Saiful Islam, Una Kjallquist, Annalena Moliner, Pawel Zajac, Jian-Bing Fan, Peter Lonnerberg, and Sten Linnarsson. Characterization of the single-cell transcriptional landscape by highly multiplex RNA-seq. Genome research.

## Simulating from zingeR framework

To simulate a scRNA-seq dataset, we use the simulation framework described in zingeR paper (http://www.biorxiv.org/content/early/2017/06/30/157982). Positive and zero counts are separately simulated using a Hurdle model.

- Positive counts. Removing the zeros, mean expression and dispersion are estimated genewise ($\lambda_g$ and $\phi_g$). Then, these estimated parameters are used to simulate positive counts using a ZTNB with mean $\hat \mu_{gi} = \hat \lambda_{gi} N_i$ and dispersion $\hat \phi_g$ where $N_i$ is the library size samples from the real dataset.

- Zero counts: for each gene, simulated using a binomial distribution with parameters estimated from real dataset $p_g = \frac{\sum_{i=1}^n p_{gi}}{n}$ where $p_{ig}$ is estimated from a function depending on the average log CPM $A_g$ of a gene and the library size $N_i$ (semiparametric additive logistic regression model).



```{r data}
#code from https://github.com/statOmics/zingeR/blob/master/vignettes/zingeRVignette_v2.Rmd
data(islamEset, package = "zingeR")
islamHlp=exprs(islamEset)[9:nrow(exprs(islamEset)),] #first 8 are spike-ins.
cellType=pData(islamEset)[,"cellType"]
paramsIslam = getDatasetMoMPositive(counts = islamHlp)
```

```{r sims}
# code from https://github.com/statOmics/zingeR/blob/master/vignettes/zingeRVignette_v2.Rmd
nSamples=80
grp=as.factor(rep(0:1, each = nSamples/2)) #two-group comparison
nTags=10000 #nr of features
set.seed(11)
DEind = sample(1:nTags,floor(nTags*.1),replace=FALSE) #10% DE
fcSim=(2 + rexp(length(DEind), rate = 1/2)) #fold changes
libSizes=sample(colSums(islamHlp),nSamples,replace=TRUE) #library sizes
simDataIslam <- NBsimSingleCell(foldDiff = fcSim, ind = DEind,
                                dataset = islamHlp, nTags = nTags,
                                group = grp,
                                verbose = TRUE, params = paramsIslam,
                                lib.size = libSizes, normalizeLambda=TRUE)
simDataIslam$counts[1:5,1:5]

# BCV plots
dOrig=suppressWarnings(edgeR::calcNormFactors(DGEList(islamHlp)))
dOrig=estimateGLMTagwiseDisp(estimateGLMCommonDisp(dOrig, design=model.matrix(~cellType), interval=c(0,10)),prior.df=0)

d=suppressWarnings(edgeR::calcNormFactors(DGEList(simDataIslam$counts)))
d=estimateGLMTagwiseDisp(estimateGLMCommonDisp(d, design=model.matrix(~grp), interval=c(0,10)),prior.df=0)

par(mfrow=c(1,2))
plotBCV(dOrig,ylim=c(0,13), xlim=c(2,16), main="real dataset")
plotBCV(d,ylim=c(0,13), xlim=c(2,16), main="simulated dataset")
par(mfrow=c(1,1))

# association of library size with zeros
plot(x=colSums(islamHlp), y=colMeans(islamHlp==0), xlab="Log library size", ylab="Fraction of zeros", ylim=c(0.2,1))
points(x=colSums(simDataIslam$counts), y=colMeans(simDataIslam$counts==0), col=2)
legend("bottomleft", c("real", "simulated"), col=1:2, pch=1)

# association of aveLogCPM with zeros
plot(x=edgeR::aveLogCPM(islamHlp), y=rowMeans(islamHlp==0), xlab="Average log CPM", ylab="Fraction of zeros", ylim=c(0,1), col=alpha(1,1/2), pch=19, cex=.3)
points(x=edgeR::aveLogCPM(simDataIslam$counts), y=rowMeans(simDataIslam$counts==0),col=alpha(2,1/2),pch=19,cex=.3)
legend("bottomleft", c("real", "simulated"), col=1:2, pch=16)

```

# Methods
## RNA-seq methods
### edgeR
```{r edgeR}
edgeR <- function(counts, group, ylim = NULL, xlim = NULL){
  d <- DGEList(counts)
  d <- suppressWarnings(edgeR::calcNormFactors(d))
  design <- model.matrix(~group)
  d <- estimateDisp(d, design)
  plotBCV(d, ylim = ylim, main = 'edgeR', xlim = xlim)
  fit <- glmFit(d,design)
  lrt <- glmLRT(fit, coef = 2)
  pval <- lrt$table$PValue
  padj <- p.adjust(pval, "BH")
  cbind(pval = pval, padj = padj)
}
```

### DESeq2
```{r DESeq2}
DESeq2 <- function(counts, group, ylim = NULL, xlim = NULL){
  colData <- data.frame(group = group)
  dse <- DESeqDataSetFromMatrix(countData=counts, colData=colData, design=~group)
  colData(dse)$group <- as.factor(colData(dse)$group)
  dse <- DESeq2::estimateSizeFactors(dse, type="poscounts")
  dse <- estimateDispersions(dse)
  dse <- nbinomWaldTest(dse, betaPrior=TRUE)
  rr <- results(dse)
  cbind(pval = rr$pvalue, padj = rr$padj)
}
```

### limma-voom
```{r limma}
limma <- function(counts, group, ylim = NULL, xlim = NULL){
	design <- model.matrix(~ group)
	nf <- suppressWarnings(edgeR::calcNormFactors(counts))
	y <- voom(counts, design, plot = FALSE, lib.size = colSums(counts) * nf)
	fit <- lmFit(y, design)
	fit <- eBayes(fit)
	tt <- topTable(fit, coef = 2, n = nrow(counts), sort.by = "none")
	pval <- tt$P.Value
	padj <- tt$adj.P.Val
	cbind(pval = pval, padj = padj)
}
```

## scRNA-seq methods

### scde

We encounter errors with the latest version of scde, as documented here: https://groups.google.com/forum/#!topic/singlecellstats/rbFUTOQ9wu4. We followed the guidelines suggested by the authors and work with version 1.99.2.

```{r scde}
scde <- function(counts, group, ylim=NULL, xlim=NULL){
  counts = matrix(as.integer(counts),nrow=nrow(counts),ncol=ncol(counts))
  if(is.null(colnames(counts))) colnames(counts)=paste0("sample",1:ncol(counts))
  require(scde)

  # calculate error models
  o.ifm <- scde.error.models(counts = counts, groups = group, n.cores = 1, threshold.segmentation = TRUE, save.crossfit.plots = FALSE, save.model.plots = FALSE, verbose = 0)
  # estimate gene expression prior
  o.prior <- scde.expression.prior(models = o.ifm, counts = counts, length.out = 400, show.plot = FALSE)
  # calculate differential expression
  ediff <- scde.expression.difference(o.ifm, counts, o.prior, groups  =  group, n.randomizations  =  150, n.cores  =  1, verbose  =  0)
  lfc <- ediff$mle
  pval=(1-pnorm(abs(ediff$Z)))*2
  padj=(1-pnorm(abs(ediff$cZ)))*2
  out = cbind(pval,padj,lfc)
  return(out)
}

```

### MAST

```{r MAST}
### copied code from FPR_mocks.Rmd on September 14, 2017.
MAST <- function(counts, group, ylim = NULL, xlim = NULL){
  require(MAST)
  tpm <- counts*1e6/colSums(counts)
  tpm <- log2(tpm+1)
  sca <- FromMatrix(tpm,  cData=data.frame(group=group))
  #sca <- FromMatrix(counts,  cData=data.frame(group=group))

  # Adaptive thresholding from MAST vignette
  freq_expressed <- 0.2
  thres <- thresholdSCRNACountMatrix(assay(sca), nbins = 10, min_per_bin = 50, conditions = group)
  #par(mfrow=c(5,4))
  #plot(thres)
  assays(sca) <- list(thresh=thres$counts_threshold, tpm=assay(sca))
  expressed_genes <- freq(sca) > freq_expressed
  sca <- sca[expressed_genes,]

  ngeneson <- apply(counts,2,function(x) mean(x>0))
  CD <- colData(sca)
  CD$ngeneson <- ngeneson
  CD$cngeneson <- CD$ngeneson-mean(ngeneson)
  colData(sca) <- CD
  ## differential expression
  fit <- zlm(~ cngeneson + group , sca = sca)
  lrFit <- lrTest(fit, 'group')
  pval <- lrFit[, 'hurdle', 'Pr(>Chisq)']
  padj <- p.adjust(pval, method = "BH")

  ### MAST filtered the genes, so make a list that is consistent with the original count matrix.
  pvalAll = vector(length=nrow(counts))
  pvalAll[] = 1
  names(pvalAll)=rownames(counts)
  pvalAll[match(names(pval),names(pvalAll))] = pval

  padjAll = vector(length=nrow(counts))
  padjAll[] = 1
  names(padjAll)=rownames(counts)
  padjAll[match(names(padj),names(padjAll))] = padj

  out = cbind(pval = pvalAll, padj = padjAll, logfc = NA)
  return(out)
}

```

### NODES

```{r}
NODES <- function(counts, group, xlim, ylim){
  require(NODES)
  g=ifelse(group==0,"A","B")
  colnames(counts)=g
  normCounts=pQ(counts)
  res=NODES::NODES(data=normCounts,group=colnames(normCounts))
  pval=vector(length=nrow(counts))
  names(pval)=rownames(counts)
  pval[rownames(normCounts)]=res$Fisher
  pval[is.na(pval)]=1
  padj=p.adjust(pval,"BH")
  lfc=NA
  out=cbind(pval,padj,lfc)
  return(out)
}
```

### metagenomeSeq

```{r metagenomeSeq}
metagenomeSeq <- function(counts, group, xlim, ylim){
  require(metagenomeSeq)
  design <- model.matrix(~group)
  pheno <- AnnotatedDataFrame(data.frame(group=group))
  rownames(pheno) <- colnames(counts)
  p <- cumNormStatFast(counts)
  dat <- newMRexperiment(counts=counts, phenoData=pheno, featureData = NULL, libSize = colSums(counts), normFactors = metagenomeSeq::calcNormFactors(counts, p=p))
  fit <- fitZig(dat,design)
  lfc <- fit$eb$coefficients[,"group1"]
  pval <- fit$eb$p.value[,"group1"]
  padj <- p.adjust(pval)
  out <- cbind(pval,padj,lfc)
  return(out)
}
```

```{r seurat}
Seurat <- function(counts, group, xlim=NULL, ylim=NULL){
    require(Seurat)
    seur = CreateSeuratObject(counts, project=paste0("cell",as.character(group)), display.progress = FALSE)
    res <- FindMarkers(seur, ident.1 = "cell0", ident.2 = "cell1", print.bar=FALSE)
    pval = res$p_val[match(rownames(counts),rownames(res))]
    padj = p.adjust(pval,"BH")
    out=cbind(pval,padj)
    return(out)
}
```

## zingeR

Counts are modelled as ZINB. Weights are posterior probabilities that a count belongs to the count component given that the count and library size is observed. Parameters are estimated using EM algorithm. See http://www.biorxiv.org/content/early/2017/06/30/157982 for more details.

### zingeR-edgeR
```{r zingeR-edgeR}
zingeR_edgeR <- function(counts, group, ylim = NULL, xlim = NULL){
  require(zingeR)
  d <- DGEList(counts)
  d <- suppressWarnings(edgeR::calcNormFactors(d))
  design <- model.matrix(~ group)
  weights <- zeroWeightsLS(counts = d$counts, design = design, maxit = 200,
                           normalization = "TMM", verbose = F)
  d$weights <- weights
  d <- estimateDisp(d, design)
  plotBCV(d, ylim = ylim, main = 'zingeR', xlim = xlim)
  fit <- glmFit(d,design)
  lrt <- glmWeightedF(fit, coef = 2, independentFiltering = TRUE)
  cbind(pval = lrt$table$PValue, padj =lrt$table$padjFilter)
}
```

### zingeR-DESeq2
```{r zingeR-deseq2}
zingeR_DESeq2 <- function(counts, group, ylim = NULL, xlim = NULL){
  require(zingeR)
  colData <- data.frame(group = group)
  design <- model.matrix(~ group)
  dse <- DESeqDataSetFromMatrix(countData = counts, colData = colData,
                                design = ~group)
  weights <- zeroWeightsLS(counts = counts, design = design, maxit = 200,
                           normalization = "DESeq2_poscounts", colData = colData,
                           designFormula = ~group, verbose = F)
  assays(dse)[["weights"]] <- weights
  dse <- DESeq2::estimateSizeFactors(dse, type="poscounts")
  dse <- estimateDispersions(dse)
  dse <- nbinomWaldTest(dse, betaPrior = TRUE, useT = TRUE,
                        df = rowSums(weights) - 2)
  rr <- results(dse)
  cbind(pval = rr$pvalue, padj = rr$padj)
}
```

### zingeR-limma-voom-filtered
```{r zinger-limmavoom}
zingeR_limma <- function(counts, group, ylim = NULL, xlim = NULL){
  require(zingeR)
  design <- model.matrix(~group)
  nf <- edgeR::calcNormFactors(counts)
  zeroWeights <- zeroWeightsLS(counts=counts, design=design, maxit = 200,
                               verbose = FALSE)
  y <- voom(counts, design, plot=FALSE, lib.size = colSums(counts)*nf,
            weights = zeroWeights)
  y$weights <- y$weights*zeroWeights
  fit <- lmFit(y, design, weights=y$weights)
  fit$df.residual <- rowSums(zeroWeights) - ncol(design)
  fit <- eBayes(fit)
  tt <- topTable(fit,coef=2,n=nrow(counts), sort.by = "none")
  pval <- tt$P.Value
  baseMean = unname(rowMeans(sweep(counts,2,nf,FUN="*")))
  hlp <- pvalueAdjustment_kvdb(baseMean=baseMean, pValue=pval)
  padj <- hlp$padj
  cbind(pval = pval, padj = padj)
}
```

## zinbwave

We compute the same weights as zingeR (i.e. posterior probabilities that a count belongs to the count component given that the count and library size is observed), but using the ZINB-WaVE estimation procedure. See more details here (http://www.biorxiv.org/content/early/2017/04/06/125112).

```{r zinbwaveWeights}
computeZinbwaveWeights <- function(zinb, counts){
  mu <- getMu(zinb)
  pi <- getPi(zinb)
  theta <- getTheta(zinb)
  theta_mat <- matrix(rep(theta, each = ncol(counts)), ncol = nrow(counts))
  nb_part <- dnbinom(t(counts), size = theta_mat, mu = mu)
  zinb_part <- pi * ( t(counts) == 0 ) + (1 - pi) *  nb_part
  zinbwg <- ( (1 - pi) * nb_part ) / zinb_part
  t(zinbwg)
}
```

### zinbwave-edgeR
```{r zinbwaveedger}
zinbwave_edgeR <- function(counts, group, zinb, ylim = NULL, xlim = NULL, main = 'ZINB-WaVE'){
  d=DGEList(counts)
  d=suppressWarnings(edgeR::calcNormFactors(d))
  design=model.matrix(~group)
  weights <- computeZinbwaveWeights(zinb, d$counts)
  d$weights <- weights
  d=estimateDisp(d, design)
  plotBCV(d, ylim = ylim, main = main)
  fit=glmFit(d,design)
  lrt=zingeR::glmWeightedF(fit,coef=2, independentFiltering = TRUE)
  cbind(pval = lrt$table$PValue, padj =lrt$table$padjFilter)
}
```

### zinbwave-DESeq2
```{r zinbwavedeseq2}
zinbwave_DESeq2 <- function(counts, group, zinb){
  colData=data.frame(group=group)
  design=model.matrix(~group)
  dse=DESeqDataSetFromMatrix(countData=counts, colData=colData, design=~group)
  weights <- computeZinbwaveWeights(zinb, counts(dse))
  weights[weights<1e-6] = 1e-6 #prevent sanity check error for weights
  assays(dse)[["weights"]]=weights
  dse = DESeq2::estimateSizeFactors(dse, type="poscounts")
  dse = estimateDispersions(dse)
  dse = nbinomWaldTest(dse, betaPrior=TRUE, useT=TRUE, df=rowSums(weights)-2)
  res = results(dse)
  cbind(pval = res$pvalue, padj = res$padj)
}
```

### zinbwave-limma-voom
```{r zinbwavevoom}
zinbwave_limma <- function(counts, group, zinb){
  design <- model.matrix(~group)
  nf <- edgeR::calcNormFactors(counts)
  zeroWeights <- computeZinbwaveWeights(zinb, counts)
  y <- voom(counts, design, plot=FALSE, lib.size = colSums(counts)*nf,
            weights = zeroWeights)
  y$weights <- y$weights * zeroWeights
  fit <- lmFit(y, design, weights=y$weights)
  fit$df.residual <- rowSums(zeroWeights) - ncol(design)
  fit <- eBayes(fit)
  tt <- topTable(fit,coef=2,n=nrow(counts), sort.by = "none")
  pval <- tt$P.Value
  baseMean = unname(rowMeans(sweep(counts,2,nf,FUN="*")))
  hlp <- pvalueAdjustment_kvdb(baseMean=baseMean, pValue=pval)
  padj <- hlp$padj
  cbind(pval = pval, padj = padj)
}
```

# Results

```{r core}
core <- SummarizedExperiment(simDataIslam$counts,
                             colData = data.frame(grp = grp))
```

```{r zinbcommondisp}
#zinb_c <- zinbFit(core, X = '~ grp', commondispersion = TRUE, epsilon=1e12)
#save(zinb_c, file = 'zinb-common-disp-fc2-eps12.rda')
load('/Users/koenvandenberge/Dropbox/phdKoen/singleCell/zinbwaveZingerGithub/zinbwaveZinger/zinbwaveSimulations/islam_sims_fc2/zinb-common-disp-fc2-eps12.rda')
```

```{r zinbgenewisedisp}
#zinb_g <- zinbFit(core, X = '~ grp', commondispersion = FALSE, epsilon=1e12)
#save(zinb_g, file = 'zinb-genewise-disp-fc2.rda')
load('/Users/koenvandenberge/Dropbox/phdKoen/singleCell/zinbwaveZingerGithub/zinbwaveZinger/zinbwaveSimulations/islam_sims_fc2/zinb-genewise-disp-fc2.rda')
```

# Compare dispersion estimates
```{r islamDispFC2, warning=FALSE}
counts = simDataIslam$counts
myfct = list(DESeq2 = DESeq2,
             edgeR = edgeR,
             limmavoom = limma,
             MAST = MAST,
             NODES = NODES,
             scde = scde,
             metagenomeSeq = metagenomeSeq)
# if we additionally load Seurat, too many packages are loaded and the DLL limit is reached. We ran Seurat in a separate session and will add it in this session.


par(mfrow = c(2,2))
ylim = c(0, 11)
xlim = c(0, 16)
res = lapply(myfct, function(fct){
  fct(counts = counts, group = grp, ylim = ylim, xlim = xlim)
})
load("~/seuratResIslam.rda")
res[[8]] = seuratRes
names(res)[8] = "Seurat"
res[['ZINB-WaVE_DESeq2_common']] = zinbwave_DESeq2(counts, grp, zinb_c)
res[['ZINB-WaVE_edgeR_common']]  = zinbwave_edgeR(counts, grp, zinb_c, ylim=ylim, main = 'ZINB-WaVE, common dispersion', xlim = xlim)
res[['ZINB-WaVE_limmavoom_common']]  = zinbwave_limma(counts, grp, zinb_c)
res[['ZINB-WaVE_DESeq2_genewise']] = zinbwave_DESeq2(counts, grp, zinb_g)
res[['ZINB-WaVE_edgeR_genewise']]  = zinbwave_edgeR(counts, grp, zinb_g, ylim=ylim, main = 'ZINB-WaVE, genewise dispersion', xlim = xlim)
res[['ZINB-WaVE_limmavoom_genewise']]  = zinbwave_limma(counts, grp, zinb_g)
par(mfrow = c(1,1))
```

```{r res}
#load("resIslamFc2.rda")
res = lapply(res, as.data.frame)
for(k in 1:length(res)) res[[k]]$padj[is.na(res[[k]]$padj)] = 1
```

## Compare weights estimates

<!-- ```{r zingerEdgerWeights}
d=DGEList(simDataIslam$counts)
d=suppressWarnings(calcNormFactors(d))
design=model.matrix(~grp)
zingeR_edgeR_weights <- zeroWeightsLS(counts=d$counts, design=design,
                                      normalization="TMM", verbose = F)
```

```{r zingerDESeq2Weights}
colData <- data.frame(grp = grp)
design <- model.matrix(~ grp)
zingeR_DESeq2_weights <- zeroWeightsLS(counts = counts, design = design,
                           normalization = "DESeq2_poscounts", colData = colData,
                           designFormula = ~grp, verbose = F)
``` -->

```{r zinbwaveW}
zinbwave_c_weights <- computeZinbwaveWeights(zinb_c, counts)
zinbwave_g_weights <- computeZinbwaveWeights(zinb_g, counts)
```

```{r islamWeightsFC2}
par(mfrow=c(1,2))
#hist(zingeR_edgeR_weights, main='zingeR_edgeR', xlab = 'Weights')
#hist(zingeR_DESeq2_weights, main='zingeR_DESeq2', xlab = 'Weights')
hist(zinbwave_c_weights, main ='ZINB-WaVE, common dispersion', xlab = 'Weights')
hist(zinbwave_g_weights, main ='ZINB-WaVE, genewise dispersion', xlab = 'Weights')
par(mfrow=c(1,1))
```

```{r qqplotFC2}
qqplot(zinbwave_c_weights, zinbwave_g_weights, type = 'o',
       main = '',
       xlab = 'ZINB-WaVE weights, common dispersion',
       ylab = 'ZINB-WaVE weights, genewise dispersion')
abline(a=0,b=1)
```


## nDE, TPR, FDR (pvalue = 0.05)
```{r islamTableFC2, results = 'asis'}
listRates = lapply(res, function(y){
  nDE = sum(y$padj <= 0.05, na.rm = TRUE)
  TPR = mean(simDataIslam$indDE %in% which( y$padj <= 0.05))
  FDR = mean(which(y$padj <= 0.05) %in% simDataIslam$indNonDE)
  c(nDE = nDE, TPR = TPR, FDR = FDR)
})

df = do.call(rbind, listRates)
df = as.data.frame(df)
df$Method = names(res)
df$nDE = as.integer(df$nDE)
df$TPR = round(df$TPR*100, 1)
df$FDR = round(df$FDR*100, 1)
df = df[,c('Method', 'nDE', 'TPR', 'FDR')]
colnames(df) = c('Method', 'nDE', 'TPR(%)', 'FDR(%)')
rownames(df) = NULL
kable(df)
```

## TPR vs FDR
```{r truth}
trueDE = rep(0, nTags)
trueDE[simDataIslam$indDE] = 1
```

```{r islamROCfc2zinbwave}
# reszinb = res[c('ZINB-WaVE_DESeq2_common', 'ZINB-WaVE_edgeR_common',
#                  'ZINB-WaVE_limmavoom_common', 'ZINB-WaVE_DESeq2_genewise',
#                  'ZINB-WaVE_edgeR_genewise', 'ZINB-WaVE_limmavoom_genewise')]
#
# pp = COBRAData(pval = as.data.frame(do.call(cbind, lapply(reszinb, '[[', 1))),
#                padj = as.data.frame(do.call(cbind, lapply(reszinb, '[[', 2))),
#                truth = data.frame(status = trueDE))
# cobraperf <- calculate_performance(pp, binary_truth = "status", thrs = 0.05)
# cobraplot <- prepare_data_for_plot(cobraperf, colorscheme = "Paired",
#                                    facetted = FALSE)
# plot_fdrtprcurve(cobraplot, plottype = c("curve", "points"),
#                  pointsize = .2, linewidth = .5, xaxisrange = c(0, .5)) +
#   scale_color_manual(labels = sort(names(reszinb)), values = brewer.pal(6, "Paired"),
#                      name = 'Method') + theme(legend.text=element_text(size=7)) +
#   theme(axis.text.x = element_text(size = 10),
#         axis.text.y = element_text(size = 10),
#         axis.title.x = element_text(size = 15),
#         axis.title.y = element_text(size = 15))
```

```{r islamROCfc2}
#all methods
pp = COBRAData(pval = as.data.frame(do.call(cbind, lapply(res, '[[', 1))),
               padj = as.data.frame(do.call(cbind, lapply(res, '[[', 2))),
                truth = data.frame(status = trueDE))
cobraperf <- calculate_performance(pp, binary_truth = "status", thrs = 0.05)
colors=c(limmavoom="blue", "ZINB-WaVE_limmavoom_common"="steelblue", "ZINB-WaVE_limmavoom_genewise"="darkslategray3", edgeR="red", "ZINB-WaVE_edgeR_common"="salmon", "ZINB-WaVE_edgeR_genewise"="deeppink2",  DESeq2="brown",  "ZINB-WaVE_DESeq2_common"="darkseagreen", "ZINB-WaVE_DESeq2_genewise"="darkkhaki",  MAST="darkturquoise", metagenomeSeq="forestgreen", scde="grey", NODES="black",  Seurat="dodgerblue")
#iCOBRA converts '-' to '.'. Redo this.
cobraNames = sort(names(cobraperf@overlap)[1:(ncol(cobraperf@overlap)-1)])
cobraNames = gsub(x=cobraNames, pattern=".", fixed=TRUE, replacement="-")
colsCobra=colors[match(cobraNames,names(colors))]
cobraplot <- prepare_data_for_plot(cobraperf, colorscheme=colsCobra)
plot_fdrtprcurve(cobraplot, pointsize=1)

#only common disp ZINB-WaVE
pvalDf = as.data.frame(do.call(cbind, lapply(res, '[[', 1)))
padjDf = as.data.frame(do.call(cbind, lapply(res, '[[', 2)))
pp = COBRAData(pval = pvalDf[,-grep(x=colnames(pvalDf), pattern="genewise")],
               padj = padjDf[,-grep(x=colnames(padjDf), pattern="genewise")],
                truth = data.frame(status = trueDE))
cobraperf <- calculate_performance(pp, binary_truth = "status", thrs = 0.05)
colors=c(limmavoom="blue", "ZINB-WaVE_limmavoom_common"="steelblue", "ZINB-WaVE_limmavoom_genewise"="darkslategray3", edgeR="red", "ZINB-WaVE_edgeR_common"="salmon", "ZINB-WaVE_edgeR_genewise"="deeppink2",  DESeq2="brown",  "ZINB-WaVE_DESeq2_common"="darkseagreen", "ZINB-WaVE_DESeq2_genewise"="darkkhaki",  MAST="darkturquoise", metagenomeSeq="forestgreen", scde="grey", NODES="black",  Seurat="dodgerblue")
#iCOBRA converts '-' to '.'. Redo this.
cobraNames = sort(names(cobraperf@overlap)[1:(ncol(cobraperf@overlap)-1)])
cobraNames = gsub(x=cobraNames, pattern=".", fixed=TRUE, replacement="-")
colsCobra=colors[match(cobraNames,names(colors))]
cobraplot <- prepare_data_for_plot(cobraperf, colorscheme=colsCobra)
save(cobraplot,file="cobraplotIslam.rda")
plot_fdrtprcurve(cobraplot, pointsize=1)




# res10 = res[1:10]
# names(res10) = gsub('_common', '', names(res10))
# pp = COBRAData(pval = as.data.frame(do.call(cbind, lapply(res10, '[[', 1))),
#                padj = as.data.frame(do.call(cbind, lapply(res10, '[[', 2))),
#                truth = data.frame(status = trueDE))
# cobraperf <- calculate_performance(pp, binary_truth = "status", thrs = 0.05)
#
# reds = brewer.pal(11, "RdYlGn")[1:3]
# blues = rev(brewer.pal(11, "RdYlBu"))[1:3]
# brown =  brewer.pal(8, "Dark2")[4]
# greens = rev(brewer.pal(11, "PiYG"))[1:3]
# mycol = c(blues[1], greens[1], reds[1], brown, blues[2], greens[2], reds[2],
#           blues[3], greens[3], reds[3], 'black')
# names(mycol) = c(names(res10), 'truth')
# names(cobraperf@overlap) = names(mycol)
# colsCobra <- mycol[match(sort(names(cobraperf@overlap)[1:(ncol(cobraperf@overlap)-1)]), names(mycol))]
# cobraplot <- prepare_data_for_plot(cobraperf, colorscheme = colsCobra,
#                                    facetted = FALSE)
#
# p1 <- plot_fdrtprcurve(cobraplot, plottype = c("curve", "points"), pointsize = 1,
#                        linewidth = .5, xaxisrange = c(0, .5)) +
#   theme(axis.text.x = element_text(size = 10),
#         axis.text.y = element_text(size = 10),
#         axis.title.x = element_text(size = 15),
#         axis.title.y = element_text(size = 15),
#         legend.text=element_text(size=7)) + theme(legend.position="none")
#
# orderLegend = c(2, 9, 6, 1, 8, 5, 3, 10, 7, 4)
# p2 <- plot_fdrtprcurve(cobraplot, plottype = c("curve", "points"), pointsize = 1,
#                        linewidth = .5, xaxisrange = c(0, .5)) +
#   theme(legend.text=element_text(size=7)) +
#   scale_color_manual(labels = names(colsCobra)[orderLegend],
#                      values = unname(colsCobra)[orderLegend],
#                      name = 'Method')
# legend <- get_legend(p2)
#
# #pdf("../../draftOverleaf/10963157vrrwqjqjdrnf/performanceIslamfc2.pdf")
# plot_grid(p1, legend, nrow = 1, ncol = 2, rel_widths = c(1, .4))
# #dev.off()

```

## Distribution of pvalues

```{r islamPvalues}
ylim = c(0, 3000)
par(mfrow = c(3,3))
hist = lapply(1:length(res), function(i){
  hist(res[[i]][,'pval'], main = names(res)[i], ylim = ylim, xlab = 'pvalues')
})
par(mfrow = c(1,1))
```


# JUNK


```{r junkKoen}
#
# # getExprFraction4 = function(counts, offset){
# #     countsModel = counts[counts>0]
# #     offsetModel = offset[counts>0]
# #   sum(countsModel)/sum(offsetModel)
# # }
#
# getExprFraction4 = function(counts, offset){
#     countsModel = counts[counts>0]
#     offsetModel = offset[counts>0]
#     lambda = sum(countsModel)/sum(offsetModel)
#     lambda*(1-mean(counts==0))
# }
#
# # getPhiMoMPositive4 = function(counts, lambda, offset){
# #     countsModel = counts[counts>0]
# #     offsetModel = offset[counts>0]
# #     mu=lambda*offsetModel
# #     phi = (sum(countsModel^2) - sum(mu^2) - sum(mu)) / sum(mu^2)
# #     return(phi)
# # }
#
# getPhiMoMPositive4 = function(counts, lambda, offset){
#     countsModel = counts[counts>0]
#     offsetModel = offset[counts>0]
#     mu=lambda*offsetModel
#     phi = (sum(countsModel^2 * (1-mean(counts==0))) - sum(mu^2) - sum(mu)) / sum(mu^2)
#     return(phi)
# }
#
#
# # reEstimateExprFraction4 = function(counts, offset, lambda, phi){
# #     countsModel = counts[counts>0]
# #     offsetModel = offset[counts>0]
# #     mu=lambda*offsetModel
# #   sum(countsModel*(1-dnbinom(0,mu=mu,size=1/phi)))/sum(offsetModel)
# # }
# #
# # reEstimatePhiMoM4 = function(counts, lambda, offset, phi){
# #     countsModel = counts[counts>0]
# #     offsetModel = offset[counts>0]
# #     mu=lambda*offsetModel
# #     phi = (sum(countsModel^2 * (1-dnbinom(0,mu=mu,size=1/phi))) - sum(mu^2) - sum(mu)) / sum(mu^2)
# #     return(phi)
# # }
#
#
#
# getDatasetMoMPositive = function(counts, drop.extreme.dispersion = 0.1, cpm= "AveLogCPM", MoMIter=10){
#
#   #### estimate lambda and overdispersion based on ZTNB.
# 	d <- DGEList(counts)
# 	cp <- cpm(d,normalized.lib.sizes=TRUE)
# 	dFiltered=d
# 	dFiltered <- edgeR::calcNormFactors(dFiltered)
#   dFiltered$AveLogCPM <- aveLogCPM(dFiltered)
#   ## estimate
#   lambdaMoM=apply(dFiltered$counts,1,function(x) getExprFraction4(counts=x, offset=colSums(dFiltered$counts)))
#   dispMoM = vector(length=nrow(dFiltered$counts))
#   for(i in 1:nrow(dFiltered$counts)) dispMoM[i] = getPhiMoMPositive4(counts=dFiltered$counts[i,], offset=colSums(dFiltered$counts), lambda=lambdaMoM[i])
#   dispMoM[dispMoM<0] = sample(dispMoM[dispMoM>0],sum(dispMoM<0),replace=TRUE) #sample from the non-zero dispersion
#
#   ### old code: iteratively estimating
#   #dispMoM[dispMoM<0] = 1
# #   for(j in 1:MoMIter){
# #   for(i in 1:nrow(dFiltered$counts)) lambdaMoM[i] = reEstimateExprFraction4(counts=dFiltered$counts[i,], offset=colSums(dFiltered$counts), phi=dispMoM[i], lambda=lambdaMoM[i])
# #   for(i in 1:nrow(dFiltered$counts)) dispMoM[i] = reEstimatePhiMoM4(counts=dFiltered$counts[i,], offset=colSums(dFiltered$counts), lambda=lambdaMoM[i], phi=dispMoM[i])
# # hist(dispMoM) ; message(paste("mean below zero",mean(dispMoM<0)))
# #   if(j < MoMIter) dispMoM[dispMoM<0] = 1 #except for last iteration
# #   }
#   ### end old code
#
#   ## assume convergence
#   params=cbind(dispMoM,lambdaMoM)
# 	rmRows = which(params[,2]>1) #impossibly high lambda
# 	rmRows2 = which(params[,2]==0) #zero lambda
# 	naRows = which(apply(params,1, function(row) any(is.na(row)))) #not fitted
# 	nonZeroDispRows = which(params[,1]<0 | params[,1]==0) #negative dispersion
# 	throwRows = c(rmRows,rmRows2,naRows,nonZeroDispRows)
#   if(length(throwRows)>0) params = params[-throwRows,]
#
# 	### estimate logistic GAM P(zero) ~ s(aveLogCPM) + logLibSize
# 	### use unfiltered data for this model.
#   require(mgcv)
# 	propZero = colMeans(counts==0)
# 	propZeroGene = rowMeans(counts==0)
# 	d <- DGEList(counts)
# 	d <- edgeR::calcNormFactors(d)
# 	avCpm <- aveLogCPM(d, normalized.lib.sizes=TRUE)
# 	cpmHist = hist(avCpm, breaks=150, plot=FALSE)
#     	breaks = cpmHist$breaks
#     	mids = cpmHist$mids
#     	midsHlp=rep(mids,ncol(d$counts))
# 	logLibSize = log(colSums(counts)*d$samples$norm.factors)
#     	logLibHlp=rep(logLibSize,each=length(mids))
# 	binHlp=sapply(breaks[-length(breaks)],function(x) avCpm>x)
#   	binId=apply(binHlp,1,function(x) max(which(x)))
# 	nonNullCounts = t(sapply(1:length(mids), function(bin){
# 			    binRows <- binId==bin
# 			    if(sum(binRows)==0) rep(0,ncol(counts)) else
# 			    if(sum(binRows)==1) (counts[which(binRows),]!=0)*1 else
# 				colSums(counts[which(binRows),]!=0)
# 	    }))
# 	nullCounts = t(sapply(1:length(mids), function(bin){
# 		    	binRows <- binId==bin
# 		    	if(sum(binRows)==0) rep(0,ncol(counts)) else
# 		    	if(sum(binRows)==1) (counts[which(binRows),]==0)*1 else
# 			    colSums(counts[which(binRows),]==0)
# 	    }))
# 	expectCounts=cbind(c(nullCounts),c(nonNullCounts))
# 	#zeroFit=mgcv::gam(expectCounts~s(midsHlp)+logLibHlp,family=binomial)
# 	zeroFit=gam(expectCounts~s(midsHlp,by=logLibHlp),family=binomial)
#
# 	### drop extreme dispersions
#   dFiltered$AveLogCPM <- aveLogCPM(dFiltered, normalized.lib.sizes=TRUE)
# 	if(length(throwRows)>0) dFiltered$AveLogCPM <- dFiltered$AveLogCPM[-throwRows]
# 	if(length(throwRows)>0) propZeroGene = propZeroGene[-throwRows]
# 	params=data.frame(dispersion=params[,1], lambda=params[,2], aveLogCpm=dFiltered$AveLogCPM, propZeroGene=propZeroGene)
# 	dispersion <- params$dispersion
# 	AveLogCPM <- params$aveLogCpm
# 	lambda <- params$lambda
# 	propZeroGene <- params$propZeroGene
#
# 	if(is.numeric(drop.extreme.dispersion))
# 	{
# 		bad <- quantile(dispersion, 1-drop.extreme.dispersion, names = FALSE, na.rm=TRUE)
# 		ids <- dispersion <= bad
# 		AveLogCPM <- AveLogCPM[ids]
# 		dispersion <- dispersion[ids]
# 		lambda <- lambda[ids]
# 		propZeroGene <- propZeroGene[ids]
# 		params <- params[ids,]
# 		dFiltered <- dFiltered[ids,]
# 	}
# 	#lambda=lambda/sum(lambda) #make sure they sum to 1
# 	dataset.AveLogCPM <- AveLogCPM
# 	dataset.dispersion <- dispersion
# 	dataset.lambda <- lambda
# 	dataset.propZeroGene <- propZeroGene
# 	dataset.lib.size <- d$samples$lib.size
# 	dataset.nTags <- nrow(d)
# 	list(dataset.AveLogCPM = dataset.AveLogCPM, dataset.dispersion = dataset.dispersion, dataset.lib.size = dataset.lib.size, dataset.nTags = dataset.nTags, dataset.propZeroFit=zeroFit, dataset.lambda=lambda, dataset.propZeroGene=propZeroGene, dataset.breaks = breaks, dataset.cpm=cpm)
# }
#
#
# NBsimSingleCell <- function(dataset, group, nTags = 10000, nlibs = length(group), lib.size = NULL, drop.low.lambda = TRUE, drop.extreme.dispersion = 0.1, pUp=.5, foldDiff=3, verbose=TRUE, ind=NULL, params=NULL, cpm="AveLogCPM", max.dispersion=400, min.dispersion=0.1, normalizeLambda=FALSE)
# {
#   require(edgeR)
#   group = as.factor(group)
#   expit=function(x) exp(x)/(1+exp(x))
#   logit=function(x) log(x/(1-x))
#
#   sample.fun <- function(object)
#   {
#     nlibs <- object$nlibs
#     nTags <- object$nTags
#     AveLogCPM <-object$dataset$dataset.AveLogCPM
#     dispersion <- object$dataset$dataset.dispersion
#     lambda <- object$dataset$dataset.lambda
#     #lambda <- (2^AveLogCPM)/1e6
#     propZeroGene <- dat$dataset$dataset.propZeroGene
#     id_r <- sample(length(AveLogCPM), nTags, replace = TRUE)
#     object$AveLogCPM <- AveLogCPM[id_r]
#     Lambda <- lambda[id_r]
#     if(normalizeLambda) Lambda <- Lambda/sum(Lambda) #normalize so they all sum to 1
#     Dispersion <- dispersion[id_r]
#     Dispersion[Dispersion>max.dispersion] = max.dispersion
#     Dispersion[Dispersion<min.dispersion] = min.dispersion
#     propZeroGene <- propZeroGene[id_r]
#     Lambda <- expandAsMatrix(Lambda, dim = c(nTags, nlibs))
#     object$Lambda <- Lambda
#     Dispersion <- expandAsMatrix(Dispersion, dim = c(nTags, nlibs))
#     object$Dispersion <- Dispersion
#     object$propZeroGene <- propZeroGene
#     object
#   }
#   diff.fun <- function(object)
#   {
#     group <- object$group
#     pUp <-  object$pUp
#     foldDiff <- object$foldDiff
#     Lambda <- object$Lambda
#     nTags <- object$nTags
#     g <- group == levels(group)[1]
#     #AveLogCPM = expandAsMatrix(object$AveLogCPM,dim=c(nTags, nlibs))
#     if(length(ind)>0 & !all(foldDiff==1)) {
#       fcDir <- sample(c(-1,1), length(ind), prob=c(1-pUp,pUp), replace=TRUE)
#       Lambda[ind,g] <- Lambda[ind,g]*exp(log(foldDiff)/2*fcDir)
#       Lambda[ind,!g] <- Lambda[ind,!g]*exp(log(foldDiff)/2*(-fcDir))
#       object$Lambda <- Lambda
#       object$indDE <- ind
#       object$indNonDE <- (1:nTags)[-ind]
#       foldDiff[fcDir==1] <- 1/foldDiff[fcDir==1]
#       object$foldDiff <- foldDiff #group2 / group1
#     }
#     if(all(foldDiff==1)) object$indDE <- NA
#     object
#   }
#   sim.fun <- function(object)
#   {
#     Lambda <- object$Lambda
#     Dispersion <- object$Dispersion
#     nTags <- object$nTags
#     nlibs <- object$nlibs
#     lib.size <- object$lib.size
#     zeroFit <- dat$dataset$dataset.propZeroFit
#     propZeroGene <- dat$propZeroGene
#     propZeroGene[propZeroGene==1] <- 1-1e-4
#     propZeroGene[propZeroGene==0] <- 1e-4
#     design <- object$design
#     avLogCpm <- object$AveLogCPM
#     mids <- object$dataset$dataset.mids
#     breaks <- object$dataset$dataset.breaks
#
#     ## get matrix of zero probabilities
#     libPredict=rep(log(lib.size),each=length(avLogCpm))
#     cpmPredict=rep(avLogCpm,length(lib.size))
#     zeroProbMatLink = matrix(predict(zeroFit, newdata=data.frame(logLibHlp=libPredict, midsHlp=cpmPredict), type="link"), byrow=FALSE, ncol=nlibs, nrow=nTags)
#     meanDiff = rowMeans(zeroProbMatLink)-logit(propZeroGene)
#     zeroProbMat = expit(sweep(zeroProbMatLink,1,meanDiff,"-"))
#     #zeroProbMat = matrix(predict(zeroFit, newdata=data.frame(logLibHlp=libPredict, midsHlp=cpmPredict), type="response"), byrow=FALSE, ncol=nlibs)
#
#     ## simulate negative binomial counts
#     mu=sweep(Lambda,2,lib.size,"*")
#     #adjustment = zeroProbMat*mu
#     #mu=mu+adjustment
#     #mu[mu<0.1] = 0.1
#     #counts = matrix(rnbinom(n=nTags*nlibs, mu=mu, size=1/Dispersion), nrow=nTags, ncol=nlibs, byrow=FALSE)
#     zeroProbNegBin = matrix(dnbinom(0, mu=mu, size=1/Dispersion), nrow=nTags, ncol=nlibs, byrow=FALSE)
#     expectedZeroProbablityNegBinomial = rowMeans(zeroProbNegBin)
#     dropoutGenes = expectedZeroProbablityNegBinomial < rowMeans(zeroProbMat)
#     adjustment = zeroProbMat*mu
#     mu[dropoutGenes,]=mu[dropoutGenes,]+adjustment[dropoutGenes,]
#     mu[mu<0.1] = 0.1
#     counts = matrix(rnbinom(n=nTags*nlibs, mu=mu, size=1/Dispersion), nrow=nTags, ncol=nlibs, byrow=FALSE)
#
#
#
#
#     ## calculate dropouts
#     message(paste0("Adding extra zeros w.r.t. NB for ",sum(dropoutGenes)," genes"))
#     #dropout matrix is 0 for dropout.
#     dropoutMatrix = 1-matrix(rbinom(n=nTags*nlibs, size=1, prob=zeroProbMat), nrow=nTags, ncol=nlibs, byrow=FALSE)
#     dropoutMatrix[!dropoutGenes,] = 1
#     #avoid all dropout genes
#     allDropoutId <- which(rowSums(dropoutMatrix)==0)
#     while(length(allDropoutId)>0 ){
#       dropoutMatrix[allDropoutId,] = 1-matrix(rbinom(n=length(allDropoutId)*nlibs, size=1, prob=zeroProbMat[allDropoutId,]), nrow=length(allDropoutId), ncol=nlibs, byrow=FALSE)
#       allDropoutId <- which(rowSums(dropoutMatrix)==0)
#     }
#     #add dropouts
#     counts = counts*dropoutMatrix
#     object$dropout = dropoutMatrix
#
#     ## resample counts for features with all zero counts
#     zeroCountsId <- which(rowSums(counts)==0)
#     while(length(zeroCountsId)>0 ){
#       counts[zeroCountsId,] = matrix(rnbinom(n=length(zeroCountsId)*nlibs, mu=mu[zeroCountsId,], size=1/Dispersion[zeroCountsId,]), nrow=length(zeroCountsId), ncol=nlibs, byrow=FALSE)
#       counts[zeroCountsId,]=counts[zeroCountsId,]*dropoutMatrix[zeroCountsId,]
#       zeroCountsId <- which(rowSums(counts)==0)
#     }
#
#     ## name features, return object.
#     rownames(counts) <- paste("ids", 1:nTags, sep = "")
#     colnames(counts) <- paste("sample",1:nlibs,sep="")
#     object$counts <- counts
#     object
#   }
#
#   if(verbose) message("Preparing dataset.\n")
#   if(is.null(params)){
#     dataset <- getDatasetZTNB(counts = dataset, drop.extreme.dispersion = drop.extreme.dispersion, drop.low.lambda = drop.low.lambda)
#   } else {
#     dataset <- params
#   }
#   dat <- new("DGEList", list(dataset = dataset, nTags = nTags, lib.size = lib.size, nlibs = nlibs, group = group, design = model.matrix(~group), pUp = pUp, foldDiff = foldDiff))
#   if(cpm=="aCpm") dat$dataset$dataset.AveLogCPM = dat$dataset$dataset.aCpm
#
#
#   if(is.null(dat$lib.size)){
#     dat$lib.size <- sample(dataset$dataset.lib.size, nlibs, replace=TRUE)}
#   if(is.null(nTags)) dat$nTags <- dat$dataset$dataset.nTags
#   if(verbose) message("Sampling.\n")
#   dat <- sample.fun(dat)
#   if(verbose) message("Calculating differential expression.\n")
#   dat <- diff.fun(dat)
#   if(verbose) message("Simulating data.\n")
#   dat <- sim.fun(dat)
#   dat
# }

```
